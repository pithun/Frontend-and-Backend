{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fadfb54-4eaa-4cca-a77a-b6852bfe16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pathlib\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from xai_sdk import Client\n",
    "from xai_sdk.chat import user\n",
    "from xai_sdk.search import SearchParameters, web_source, news_source, x_source, rss_source\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5743a16-c41c-4f80-ba05-056faa6d4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your schema\n",
    "REQUIRED_FIELDS = [\"title\", \"description\", \"state\", \"lga\", \"status\"]\n",
    "OPTIONAL_FIELDS_i = [\"incidentDate\", \"incidentTime\"]\n",
    "OPTIONAL_FIELDS_ii = [\"lat\", \"lng\"]\n",
    "ALL_FIELDS = REQUIRED_FIELDS + OPTIONAL_FIELDS_i + OPTIONAL_FIELDS_ii + [\"is_duplicate\"]\n",
    "\n",
    "\n",
    "def normalize_news(news_data):\n",
    "    \"\"\"\n",
    "    Normalize list of dicts into a DataFrame with consistent schema.\n",
    "    Missing fields are filled with None.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(news_data)\n",
    "    for col in ALL_FIELDS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None  # fill missing fields\n",
    "    # Ensure correct column order\n",
    "    df = df[ALL_FIELDS]\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_news_to_csv(news_data, folder=\"news_data\"):\n",
    "    \"\"\"Save current run news into a timestamped CSV\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    curr_dt = datetime.now()\n",
    "    timestamp = curr_dt.strftime(\"%Y%m%d_%H\")\n",
    "    exec_month = curr_dt.strftime(\"%Y-%m\")\n",
    "    filepath = os.path.join(folder, exec_month, f\"{timestamp}.csv\")\n",
    "    df = normalize_news(news_data)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"‚úÖ Saved {len(df)} news to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_recent_news(folder=\"news_data\", days=5):\n",
    "    \"\"\"Load news from the last N days of CSVs\"\"\"\n",
    "    curr_dt = datetime.now()\n",
    "    exec_month = curr_dt.strftime(\"%Y-%m\")\n",
    "    \n",
    "    cutoff = datetime.now() - timedelta(days=days)\n",
    "    dfs = []\n",
    "    for fname in os.listdir(os.path.join(folder, exec_month)):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            try:\n",
    "                file_date = datetime.strptime(fname.split(\".\")[0], \"%Y%m%d_%H\")\n",
    "                if file_date >= cutoff:\n",
    "                    dfs.append(pd.read_csv(os.path.join(folder, exec_month, fname)))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "def deduplicate_news(current_news, past_news, threshold=0.35):\n",
    "    \"\"\"Remove duplicates by cosine similarity on title+description\"\"\"\n",
    "    if past_news.empty:\n",
    "        current_news[\"is_duplicate\"] = False\n",
    "        return current_news\n",
    "\n",
    "    combined_past = (\n",
    "        past_news[\"title\"].fillna(\"\") + \" \" + past_news[\"description\"].fillna(\"\")\n",
    "    ).tolist()\n",
    "    combined_current = (\n",
    "        current_news[\"title\"].fillna(\"\") + \" \" + current_news[\"description\"].fillna(\"\")\n",
    "    ).tolist()\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit(combined_past + combined_current)\n",
    "    past_vecs = vectorizer.transform(combined_past)\n",
    "    curr_vecs = vectorizer.transform(combined_current)\n",
    "\n",
    "    duplicates = []\n",
    "    for i, vec in enumerate(curr_vecs):\n",
    "        sim = cosine_similarity(vec, past_vecs).max()\n",
    "        print(sim)\n",
    "        duplicates.append(sim >= threshold)\n",
    "\n",
    "    current_news[\"is_duplicate\"] = duplicates\n",
    "    return current_news\n",
    "\n",
    "def publish_news(api_key: str, news_items: list):\n",
    "    \"\"\"\n",
    "    Publishes news items to the Convex threats API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your Convex API key (string starting with stmp_...).\n",
    "        news_items (list): A list of dicts containing threat data. \n",
    "                           Must include required fields:\n",
    "                           title, description, state, lga, status\n",
    "                           Optional fields: incidentDate, incidentTime, lat, lng\n",
    "    \"\"\"\n",
    "    url = f\"https://fantastic-mammoth-699.convex.site/api/threats?api_key={api_key}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=news_items)\n",
    "        response.raise_for_status()\n",
    "        print(\"‚úÖ Successfully published news!\")\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"‚ùå HTTP error occurred: {http_err} - {response.text}\")\n",
    "    except Exception as err:\n",
    "        print(f\"‚ùå Other error occurred: {err}\")\n",
    "\n",
    "def fetch_security_news(api_key: str):\n",
    "    \"\"\"\n",
    "    Fetch security-related news in Nigeria from the last 8 hours.\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Your XAI API key.\n",
    "\n",
    "    Returns:\n",
    "        list | dict: Parsed JSON object with news items if successful, \n",
    "                     or None if parsing fails.\n",
    "    \"\"\"\n",
    "    client = Client(api_key=api_key)\n",
    "\n",
    "    # Dynamic dates for last 8 hours\n",
    "    from_date = datetime.now() - timedelta(hours=8)\n",
    "    to_date = datetime.now()\n",
    "\n",
    "    search_config = SearchParameters(\n",
    "        mode=\"on\",\n",
    "        return_citations=True,\n",
    "        from_date=from_date,\n",
    "        to_date=to_date,\n",
    "        max_search_results=30,\n",
    "        sources=[\n",
    "            web_source(country=\"NG\"),\n",
    "            news_source(country=\"NG\"),\n",
    "            x_source(),\n",
    "            rss_source(links=[\n",
    "                'https://news.google.com/rss/search?q=nigeria+security&hl=en-NG&gl=NG&ceid=NG:en'\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chat = client.chat.create(\n",
    "        model=\"grok-4-fast-reasoning-latest\",\n",
    "        messages=[user(\n",
    "            \"\"\"I need news from the last 8 hours related to security issues such as banditry, gunmen or any kind on violent activities\n",
    "            in Nigeria. I need the output json formatted. For each news, return like this:\n",
    "            {\"title\": \"<title>\", \n",
    "             \"description\":\"<summary>\", \n",
    "             \"state\":\"<state of occurrence>\", \n",
    "             \"lga\":\"<Please try to infer the lga from the news, the lga is compulsory>\", \n",
    "             \"incidentDate\":\"<date>\", \n",
    "             \"incidentTime\":\"<time if available>\",\n",
    "             \"status\":\"High\" or \"Medium\" or \"Low\"}.\n",
    "            If any of the fields isn't available return Null, but for date or time, \n",
    "            return current date and for time return 00:00.\"\"\"\n",
    "        )],\n",
    "        search_parameters=search_config\n",
    "    )\n",
    "\n",
    "    response = chat.sample()\n",
    "\n",
    "    try:\n",
    "        news_data = json.loads(response.content)  # Assumes valid JSON array/object\n",
    "        return news_data\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ö†Ô∏è Content is not valid JSON‚Äîcheck the raw output instead.\")\n",
    "        return None\n",
    "\n",
    "def quick_replace(val):\n",
    "    if val==None:\n",
    "        return \"Somewhere\"\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "def filter_and_publish(news_data, api_key, folder=\"news_data\"):\n",
    "    \"\"\"Workflow to save, deduplicate, and publish only unique news\"\"\"\n",
    "\n",
    "    # Load past 5 days\n",
    "    past_news = load_recent_news(folder, days=5)\n",
    "    display(past_news)\n",
    "\n",
    "    # Save current\n",
    "    filepath = save_news_to_csv(news_data, folder)\n",
    "\n",
    "    # Deduplicate\n",
    "    current_df = normalize_news(news_data)\n",
    "    deduped = deduplicate_news(current_df, past_news)\n",
    "\n",
    "    # Filter unique\n",
    "    unique_news = deduped[deduped[\"is_duplicate\"] == False]\n",
    "    unique_news['lga'] = unique_news['lga'].apply(quick_replace)\n",
    "    unique_news=unique_news[REQUIRED_FIELDS + OPTIONAL_FIELDS_i]\n",
    "\n",
    "    print(f\"üìä Found {len(unique_news)} unique news out of {len(current_df)}\")\n",
    "\n",
    "    if not unique_news.empty:\n",
    "        print(unique_news.to_dict(orient=\"records\"))\n",
    "        publish_news(api_key, unique_news.to_dict(orient=\"records\"))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No unique news to publish.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "967cc00d-8355-4927-a667-d553c8cfcfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-02 18:59:06.814392+00:00\n"
     ]
    }
   ],
   "source": [
    "from_date = datetime.now(tz=timezone.utc) - timedelta(hours=8)\n",
    "print(from_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbe4724-76b8-4594-81d5-1e00709ba58f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 298\u001b[0m\n\u001b[0;32m    295\u001b[0m grok_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m convex_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONVEX_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 298\u001b[0m news_data \u001b[38;5;241m=\u001b[39m fetch_security_news(grok_api_key)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(news_data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m news_data:\n\u001b[0;32m    301\u001b[0m     filter_and_publish(news_data, api_key\u001b[38;5;241m=\u001b[39mconvex_api_key)\n",
      "Cell \u001b[1;32mIn[1], line 234\u001b[0m, in \u001b[0;36mfetch_security_news\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    216\u001b[0m search_config \u001b[38;5;241m=\u001b[39m SearchParameters(\n\u001b[0;32m    217\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    218\u001b[0m     return_citations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     ]\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m chat \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcreate(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrok-4-fast-reasoning-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m    230\u001b[0m messages\u001b[38;5;241m=\u001b[39m[user(p)], \n\u001b[0;32m    231\u001b[0m search_parameters\u001b[38;5;241m=\u001b[39msearch_config, \n\u001b[0;32m    232\u001b[0m temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 234\u001b[0m response \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     news_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\xai_sdk\\sync\\chat.py:106\u001b[0m, in \u001b[0;36mChat.sample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Samples a single chat completion response from the model.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03mThis method sends a request to the chat API with the current conversation history\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    \"I'm doing great, thanks for asking!\"\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(\n\u001b[0;32m    102\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat.sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proto\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    103\u001b[0m     kind\u001b[38;5;241m=\u001b[39mSpanKind\u001b[38;5;241m.\u001b[39mCLIENT,\n\u001b[0;32m    104\u001b[0m     attributes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_span_request_attributes(),\n\u001b[0;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[1;32m--> 106\u001b[0m     response_pb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stub\u001b[38;5;241m.\u001b[39mGetCompletion(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    107\u001b[0m     response \u001b[38;5;241m=\u001b[39m Response(response_pb, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    108\u001b[0m     span\u001b[38;5;241m.\u001b[39mset_attributes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_span_response_attributes([response]))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_call(\n\u001b[0;32m    278\u001b[0m         request,\n\u001b[0;32m    279\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    280\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    281\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m    282\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mwait_for_ready,\n\u001b[0;32m    283\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    284\u001b[0m     )\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_interceptor.py:329\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[0;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[0;32m    331\u001b[0m )\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\xai_sdk\\client.py:180\u001b[0m, in \u001b[0;36mTimeoutInterceptor.intercept_unary_unary\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mintercept_unary_unary\u001b[39m(\u001b[38;5;28mself\u001b[39m, continuation, client_call_details, request):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Intercepts a unary-unary RPC call.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intercept_call(continuation, client_call_details, request)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\xai_sdk\\client.py:176\u001b[0m, in \u001b[0;36mTimeoutInterceptor._intercept_call\u001b[1;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_intercept_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, continuation, client_call_details, request):\n\u001b[0;32m    175\u001b[0m     client_call_details \u001b[38;5;241m=\u001b[39m client_call_details\u001b[38;5;241m.\u001b[39m_replace(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m continuation(client_call_details, request)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     new_method,\n\u001b[0;32m    308\u001b[0m     new_timeout,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m     new_compression,\n\u001b[0;32m    313\u001b[0m ) \u001b[38;5;241m=\u001b[39m _unwrap_client_call_details(new_details, client_call_details)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thunk(new_method)\u001b[38;5;241m.\u001b[39mwith_call(\n\u001b[0;32m    316\u001b[0m         request,\n\u001b[0;32m    317\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mnew_timeout,\n\u001b[0;32m    318\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mnew_metadata,\n\u001b[0;32m    319\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mnew_credentials,\n\u001b[0;32m    320\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mnew_wait_for_ready,\n\u001b[0;32m    321\u001b[0m         compression\u001b[38;5;241m=\u001b[39mnew_compression,\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:1192\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwith_call\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1190\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1191\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, grpc\u001b[38;5;241m.\u001b[39mCall]:\n\u001b[1;32m-> 1192\u001b[0m     state, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1193\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1194\u001b[0m     )\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\grpc\\_channel.py:1165\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1148\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1149\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1150\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1164\u001b[0m )\n\u001b[1;32m-> 1165\u001b[0m event \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m   1166\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pathlib\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from xai_sdk import Client\n",
    "from xai_sdk.chat import user\n",
    "from xai_sdk.search import SearchParameters, web_source, news_source, x_source, rss_source\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define your schema\n",
    "REQUIRED_FIELDS = [\"title\", \"description\", \"state\", \"lga\", \"status\"]\n",
    "OPTIONAL_FIELDS_i = [\"incidentDate\", \"incidentTime\"]\n",
    "OPTIONAL_FIELDS_ii = [\"lat\", \"lng\"]\n",
    "ALL_FIELDS = REQUIRED_FIELDS + OPTIONAL_FIELDS_i + OPTIONAL_FIELDS_ii + [\"description_with_more_context\", \"is_duplicate\"]\n",
    "\n",
    "\n",
    "def normalize_news(news_data):\n",
    "    \"\"\"\n",
    "    Normalize list of dicts into a DataFrame with consistent schema.\n",
    "    Missing fields are filled with None.\n",
    "    Validates that all required fields are present (not None) in every row.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(news_data)\n",
    "    for col in ALL_FIELDS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None  # fill missing columns\n",
    "    # Ensure correct column order\n",
    "    df = df[ALL_FIELDS]\n",
    "    \n",
    "    # Validate required fields: Check for None/NaN in REQUIRED_FIELDS\n",
    "    missing_mask = df[REQUIRED_FIELDS].isnull().any(axis=1)\n",
    "    if missing_mask.any():\n",
    "        missing_rows = df[missing_mask]\n",
    "        missing_details = missing_rows[REQUIRED_FIELDS].to_dict(orient=\"records\")\n",
    "        raise ValueError(\n",
    "            f\"Validation failed: {len(missing_rows)} news items are missing required fields. \"\n",
    "            f\"Details: {missing_details}\"\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def save_news_to_csv(news_data, folder=\"news_data\"):\n",
    "    \"\"\"Save current run news into a timestamped CSV\"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    curr_dt = datetime.now()\n",
    "    timestamp = curr_dt.strftime(\"%Y%m%d_%H\")\n",
    "    exec_month = curr_dt.strftime(\"%Y-%m\")\n",
    "    filepath = os.path.join(folder, exec_month, f\"{timestamp}.csv\")\n",
    "    df = normalize_news(news_data)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    logging.info(f\"‚úÖ Saved {len(df)} news to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_recent_news(folder=\"news_data\", days=7):\n",
    "    \"\"\"Load news from the last N days of CSVs, handling monthly folder rollover.\"\"\"\n",
    "    curr_dt = datetime.now()\n",
    "    cutoff = curr_dt - timedelta(days=days)\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    # Determine the range of months to check: from cutoff month to current month\n",
    "    current_month = curr_dt.replace(day=1)  # Start of current month\n",
    "    cutoff_month = cutoff.replace(day=1)    # Start of cutoff month\n",
    "    month = cutoff_month\n",
    "    while month <= current_month:\n",
    "        exec_month = month.strftime(\"%Y-%m\")\n",
    "        news_path = os.path.join(folder, exec_month)\n",
    "        if os.path.exists(news_path):\n",
    "            for fname in os.listdir(news_path):\n",
    "                if fname.endswith(\".csv\"):\n",
    "                    try:\n",
    "                        file_date_str = fname.split(\".\")[0]\n",
    "                        file_date = datetime.strptime(file_date_str, \"%Y%m%d_%H\")\n",
    "                        if file_date >= cutoff:\n",
    "                            dfs.append(pd.read_csv(os.path.join(news_path, fname)))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        # Move to next month\n",
    "        month += relativedelta(months=1)\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "def deduplicate_news(current_news, past_news, threshold=0.35):\n",
    "    \"\"\"Remove duplicates by cosine similarity\"\"\"\n",
    "    if past_news.empty:\n",
    "        current_news[\"is_duplicate\"] = False\n",
    "        return current_news\n",
    "\n",
    "    combined_past = (\n",
    "            past_news[\"title\"].fillna(\"\") + \" \" + \n",
    "            past_news[\"description_with_more_context\"].fillna(\"\") + \" \" + \n",
    "            past_news[\"state\"].fillna(\"\") + \" \" + \n",
    "            past_news[\"lga\"].fillna(\"\") + \" \" + \n",
    "            past_news[\"incidentDate\"].fillna(\"\")\n",
    "        ).tolist()\n",
    "    combined_current = (\n",
    "            current_news[\"title\"].fillna(\"\") + \" \" + \n",
    "            current_news[\"description_with_more_context\"].fillna(\"\") + \" \" + \n",
    "            current_news[\"state\"].fillna(\"\") + \" \" + \n",
    "            current_news[\"lga\"].fillna(\"\") + \" \" + \n",
    "            current_news[\"incidentDate\"].fillna(\"\")\n",
    "        ).tolist()\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit(combined_past + combined_current)\n",
    "    past_vecs = vectorizer.transform(combined_past)\n",
    "    curr_vecs = vectorizer.transform(combined_current)\n",
    "\n",
    "    duplicates = []\n",
    "    for i, vec in enumerate(curr_vecs):\n",
    "        sim = cosine_similarity(vec, past_vecs).max()\n",
    "        logging.info(sim)\n",
    "        duplicates.append(sim >= threshold)\n",
    "\n",
    "    current_news[\"is_duplicate\"] = duplicates\n",
    "    return current_news\n",
    "\n",
    "def publish_news(api_key: str, news_items: list):\n",
    "    \"\"\"\n",
    "    Publishes news items to the Convex threats API.\n",
    "\n",
    "    Args:\n",
    "        api_key (str): Your Convex API key (string starting with stmp_...).\n",
    "        news_items (list): A list of dicts containing threat data. \n",
    "                           Must include required fields:\n",
    "                           title, description, state, lga, status\n",
    "                           Optional fields: incidentDate, incidentTime, lat, lng\n",
    "    \"\"\"\n",
    "    url = f\"https://fantastic-mammoth-699.convex.site/api/threats?api_key={api_key}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        responses = []\n",
    "        for news_item in news_items:\n",
    "            response = requests.post(url, headers=headers, json=news_item)\n",
    "            response.raise_for_status()\n",
    "            logging.info(\"‚úÖ Successfully published news!\")\n",
    "            responses.append(response.json())\n",
    "        return responses\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logging.error(f\"‚ùå HTTP error occurred: {http_err} - {response.text}\")\n",
    "        raise\n",
    "    except Exception as err:\n",
    "        logging.error(f\"‚ùå Other error occurred: {err}\")\n",
    "        raise\n",
    "\n",
    "def fetch_security_news(api_key: str):\n",
    "    \"\"\"\n",
    "    Fetch security-related news in Nigeria from the last 8 hours.\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Your XAI API key.\n",
    "\n",
    "    Returns:\n",
    "        list | dict: Parsed JSON object with news items if successful, \n",
    "                     or None if parsing fails.\n",
    "    \"\"\"\n",
    "    client = Client(api_key=api_key)\n",
    "\n",
    "    # Dynamic dates for last 8 hours\n",
    "    from_date = datetime.now(tz=timezone.utc) - timedelta(hours=8)\n",
    "    to_date = datetime.now(tz=timezone.utc)\n",
    "\n",
    "    rss_links=[\n",
    "                'https://dailytrust.com/feed/'\n",
    "            ]\n",
    "\n",
    "    PROMPT_STRING=\"\"\"\n",
    "    Please retrieve news articles from the past 8 hours related to security issues in Nigeria, including incidents such as banditry, gunmen attacks, kidnapping, herdsmen clashes, insurgency, armed robbery, communal violence, police shootings, and other violent or criminal activities.  \n",
    "\n",
    "    The response must be returned as a valid JSON array. Each news item must follow this schema exactly:\n",
    "\n",
    "    {\n",
    "    \"title\": \"<headline of the news>\",\n",
    "    \"description\": \"<concise summary>\",\n",
    "    \"description_with_more_context\": <a more robust description of the news about 300 words is enough>\n",
    "    \"state\": \"<state of occurrence>\",\n",
    "    \"lga\": \"<local government area, compulsory ‚Äî infer if not explicitly stated>\",\n",
    "    \"incidentDate\": \"<publish date in YYYY-MM-DD; required, infer from article if possible>\",\n",
    "    \"incidentTime\": \"<time in HH:MM 24-hour format; if unavailable, use '00:00'>\",\n",
    "    \"status\": \"<one of: 'High', 'Medium', or 'Low'>\"\n",
    "    }\n",
    "\n",
    "    Rules:\n",
    "    - - 'lga' must always be inferred (e.g., from state capital if unclear), never Null.\"\n",
    "    - All fields are required. If a value cannot be found, infer it from the news content.\n",
    "    - Ensure the output is strictly valid JSON and can be parsed without errors.\n",
    "    - Do not include extra text, explanations, or formatting outside of the JSON.\n",
    "    - If there are multiple states, please create separate entries for each state and fill in an approximate local government but lga must be filled.\n",
    "    - The \"status\" field should reflect the severity of the incident based on the description (e.g., \"High\" for fatalities, \"Medium\" for injuries, \"Low\" for property damage only).\n",
    "    - Aim for diversity: Include reports from national, local, and social media sources to cover underreported areas.\n",
    "    - 'incidentDate' must be the article's publication date; only include if within past 8 hours.\"\n",
    "    \"\"\"\n",
    "\n",
    "    prompts = [\n",
    "    PROMPT_STRING.replace(\"Nigeria\", \"Northern Nigeria (e.g., banditry in Zamfara, Kaduna)\"),\n",
    "    PROMPT_STRING.replace(\"Nigeria\", \"Southern Nigeria (e.g., militancy in Delta, cultism in Rivers)\")\n",
    "    ]\n",
    "\n",
    "    all_news = []\n",
    "    for rss in rss_links:\n",
    "        for p in prompts:\n",
    "            search_config = SearchParameters(\n",
    "                mode=\"on\",\n",
    "                return_citations=True,\n",
    "                from_date=from_date,\n",
    "                to_date=to_date,\n",
    "                max_search_results=30,\n",
    "                sources=[\n",
    "                    web_source(country=\"NG\"),\n",
    "                    news_source(country=\"NG\"),\n",
    "                    x_source(),\n",
    "                    rss_source(links=[rss])\n",
    "                ]\n",
    "            )\n",
    "            chat = client.chat.create(model=\"grok-4-fast-reasoning-latest\", \n",
    "            messages=[user(p)], \n",
    "            search_parameters=search_config, \n",
    "            temperature=0)\n",
    "\n",
    "            response = chat.sample()\n",
    "            try:\n",
    "                news_data = json.loads(response.content)\n",
    "                # Filter based on full incident datetime >= from_date (naive)\n",
    "                filtered_news = []\n",
    "                from_date_naive = from_date.replace(tzinfo=None)  # Strip timezone for comparison\n",
    "                for item in news_data:\n",
    "                    date_str = item.get(\"incidentDate\", \"1900-01-01\")\n",
    "                    time_str = item.get(\"incidentTime\", \"00:00\")\n",
    "                    try:\n",
    "                        # Combine date and time into full datetime (naive)\n",
    "                        incident_dt = datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M\")\n",
    "                        if incident_dt >= from_date_naive:\n",
    "                            filtered_news.append(item)\n",
    "                    except ValueError:\n",
    "                        # Skip invalid formats (log if desired: print(f\"Skipping invalid date/time: {date_str} {time_str}\"))\n",
    "                        pass\n",
    "                \n",
    "                news_data = filtered_news\n",
    "                all_news.extend(news_data)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return all_news\n",
    "\n",
    "def quick_replace(val):\n",
    "    if val==None:\n",
    "        return \"Somewhere\"\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "def filter_and_publish(news_data, api_key, folder=\"news_data\"):\n",
    "    \"\"\"Workflow to save, deduplicate, and publish only unique news\"\"\"\n",
    "\n",
    "    # Load past 5 days\n",
    "    past_news = load_recent_news(folder, days=5)\n",
    "    logging.info(past_news)\n",
    "\n",
    "    # Save current\n",
    "    filepath = save_news_to_csv(news_data, folder)\n",
    "\n",
    "    # Deduplicate\n",
    "    current_df = normalize_news(news_data)\n",
    "    deduped = deduplicate_news(current_df, past_news)\n",
    "\n",
    "    # Filter unique\n",
    "    unique_news = deduped[deduped[\"is_duplicate\"] == False]\n",
    "    unique_news['lga'] = unique_news['lga'].apply(quick_replace)\n",
    "    unique_news=unique_news[REQUIRED_FIELDS + OPTIONAL_FIELDS_i]\n",
    "\n",
    "    logging.info(f\"üìä Found {len(unique_news)} unique news out of {len(current_df)}\")\n",
    "\n",
    "    if not unique_news.empty:\n",
    "        logging.info(unique_news.to_dict(orient=\"records\"))\n",
    "        #publish_news(api_key, unique_news.to_dict(orient=\"records\"))\n",
    "    else:\n",
    "        logging.info(\"‚ö†Ô∏è No unique news to publish.\")\n",
    "\n",
    "\n",
    "# Example integration after you parse your API response\n",
    "try:    \n",
    "    grok_api_key = os.getenv(\"XAI_API_KEY\")\n",
    "    convex_api_key = os.getenv(\"CONVEX_API_KEY\")\n",
    "    \n",
    "    news_data = fetch_security_news(grok_api_key)\n",
    "    \n",
    "    if isinstance(news_data, list) and news_data:\n",
    "        filter_and_publish(news_data, api_key=convex_api_key)\n",
    "    else:\n",
    "        logging.info(\"‚ö†Ô∏è No news items returned.\")\n",
    "except json.JSONDecodeError:\n",
    "    logging.error(\"‚ùå Could not parse response content as JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa8421-5400-407e-9797-fb0c903c6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration after you parse your API response\n",
    "try:\n",
    "    with open(\"Grok/key.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    grok_api_key = config[\"grok_api_key\"]\n",
    "    #convex_api_key = config[\"convex_api_key\"]\n",
    "    \n",
    "    news = fetch_security_news(grok_api_key)\n",
    "    print(news)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ùå Could not parse response content as JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b427464-d740-4861-b0d6-1f9559f7a536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "id: \"1046933e-de21-bfc0-f2a2-7760caec7cc7_us-east-1\"\n",
      "choices {\n",
      "  finish_reason: REASON_STOP\n",
      "  message {\n",
      "    content: \"[\\n  {\\n    \\\"title\\\": \\\"Police arrest two suspected armed robbers in Lagos, recover motorcycle\\\",\\n    \\\"description\\\": \\\"Operatives of the Ikeja Division arrested two suspects involved in armed robbery, recovering a motorcycle used in the crime.\\\",\\n    \\\"description_with_more_context\\\": \\\"In a swift response to a distress call on September 29, 2025, around 7:15 p.m., police from the Ikeja Division in Lagos State apprehended two individuals suspected of armed robbery. The operation led to the recovery of a motorcycle believed to have been used in the commission of the crime. This incident highlights ongoing efforts by law enforcement to curb rising armed robbery cases in urban areas of Lagos. No injuries or fatalities were reported during the arrest, but it underscores the persistent challenge of violent crimes in the state\\'s bustling neighborhoods. Authorities have intensified patrols in response to similar reports, aiming to restore public confidence in security measures. The suspects are currently in custody, and investigations are ongoing to uncover potential accomplices or links to other robberies in the area. This event is part of a broader pattern of criminal activities in Southern Nigeria, where economic pressures contribute to increased petty and violent crimes.\\\",\\n    \\\"state\\\": \\\"Lagos\\\",\\n    \\\"lga\\\": \\\"Ikeja\\\",\\n    \\\"incidentDate\\\": \\\"2025-10-02\\\",\\n    \\\"incidentTime\\\": \\\"19:15\\\",\\n    \\\"status\\\": \\\"Low\\\"\\n  },\\n  {\\n    \\\"title\\\": \\\"Student drowns, teachers beaten in Ogun school riot over illegal fees\\\",\\n    \\\"description\\\": \\\"Violence erupted at Ilugun High School in Abeokuta, resulting in one student\\'s death by drowning and assaults on teachers amid protests against alleged illegal fee collections.\\\",\\n    \\\"description_with_more_context\\\": \\\"On October 2, 2025, chaos broke out at Ilugun High School in Elega, Abeokuta, Ogun State, when students protested against what they perceived as unauthorized fees imposed by school management. The situation escalated last Friday when Amotekun Corps operatives were called in to disperse the crowd, leading to a riot. Tragically, one student drowned during the melee, and several teachers were beaten by enraged protesters. Property damage was also reported, including vandalized school facilities. This incident reflects deeper issues in Nigeria\\'s education sector, where financial impositions often spark unrest among underprivileged students. Local authorities have condemned the violence and promised investigations into the fee collection practices. Community leaders are calling for dialogue to prevent future occurrences, while parents express grief over the loss of life. The event has drawn attention to security lapses in educational institutions across Southern Nigeria, prompting discussions on better crowd control and student welfare. No arrests have been made yet, but police are reviewing footage to identify perpetrators. This riot adds to communal tensions in the region, exacerbated by economic hardships.\\\",\\n    \\\"state\\\": \\\"Ogun\\\",\\n    \\\"lga\\\": \\\"Abeokuta South\\\",\\n    \\\"incidentDate\\\": \\\"2025-10-03\\\",\\n    \\\"incidentTime\\\": \\\"00:00\\\",\\n    \\\"status\\\": \\\"High\\\"\\n  },\\n  {\\n    \\\"title\\\": \\\"Allegations of burning Igbo-owned shops at TradeFair in Lagos spark tensions\\\",\\n    \\\"description\\\": \\\"Reports emerge of Lagos State government actions leading to the burning of shops belonging to Igbo traders at TradeFair, amid claims of contravention of state laws and ethnic solidarity protests.\\\",\\n    \\\"description_with_more_context\\\": \\\"Social media posts and eyewitness accounts from October 2, 2025, highlight controversy at the TradeFair complex in Lagos, where shops owned by Igbo traders were allegedly set ablaze during enforcement actions by state authorities. The incident stems from violations of Lagos State laws, prompting a crackdown that traders claim was disproportionately targeted. Videos circulating online show Igbo traders reacting with chants and planks, protesting the demolitions and fires, while politicians from the Southeast reportedly intervened to show solidarity. This has fueled ethnic tensions in the commercial hub, with accusations of discrimination against non-indigenous businesses. No official confirmation of arson has been issued, but the event has disrupted trade and led to calls for compensation. It mirrors broader security concerns in Lagos, including communal violence and property disputes that often escalate into riots. Local police have increased presence to prevent further clashes, but residents fear reprisals. The TradeFair area, a key economic zone, now faces potential boycotts and legal challenges. This development underscores the fragility of inter-ethnic relations in Southern Nigeria\\'s urban centers, where economic activities intersect with identity politics. Authorities urge calm, promising investigations, but distrust lingers among affected communities.\\\",\\n    \\\"state\\\": \\\"Lagos\\\",\\n    \\\"lga\\\": \\\"Ojo\\\",\\n    \\\"incidentDate\\\": \\\"2025-10-02\\\",\\n    \\\"incidentTime\\\": \\\"00:00\\\",\\n    \\\"status\\\": \\\"Medium\\\"\\n  }\\n]\"\n",
      "    role: ROLE_ASSISTANT\n",
      "  }\n",
      "}\n",
      "created {\n",
      "  seconds: 1759459693\n",
      "  nanos: 783551857\n",
      "}\n",
      "model: \"grok-4-fast-reasoning\"\n",
      "system_fingerprint: \"fp_9362061f30\"\n",
      "usage {\n",
      "  completion_tokens: 975\n",
      "  prompt_tokens: 4720\n",
      "  total_tokens: 6917\n",
      "  prompt_text_tokens: 4720\n",
      "  reasoning_tokens: 1222\n",
      "  cached_prompt_text_tokens: 108\n",
      "  num_sources_used: 4\n",
      "}\n",
      "citations: \"https://en.wikipedia.org/wiki/2025_in_Nigeria\"\n",
      "citations: \"https://punchng.com/\"\n",
      "citations: \"https://dailytrust.com/\"\n",
      "citations: \"https://leadership.ng/\"\n",
      "citations: \"https://www.premiumtimesng.com/news/top-news/825351-police-arrest-two-suspected-armed-robbers-in-lagos-recover-motorcycle.html\"\n",
      "citations: \"https://archive.ph/fJne6\"\n",
      "citations: \"https://thisdaylive.com/2025/09/27/insecurity-need-for-a-new-strategy\"\n",
      "citations: \"https://thesoufancenter.org/intelbrief-2025-august-22/\"\n",
      "citations: \"https://vanguardngr.com/2025/09/the-case-for-establishing-state-police-in-nigeria-balancing-security-federalism-and-civil-oversight\"\n",
      "citations: \"https://jhpn.biomedcentral.com/articles/10.1186/s41043-025-00996-y\"\n",
      "citations: \"https://www.globalr2p.org/countries/nigeria/\"\n",
      "citations: \"https://www.npr.org/2025/04/15/nx-s1-5365450/nigeria-40-dead-gunmen-bola-tinubu\"\n",
      "citations: \"https://newscentral.africa/nigerias-powder-keg-a-week-of-unrelenting-violence-across-regions\"\n",
      "citations: \"https://www.npr.org/2023/02/02/1153753025/nigeria-election-ballot-boxes-destroyed\"\n",
      "citations: \"https://x.com/mydeendadon/status/1973838030789362107\"\n",
      "citations: \"https://x.com/mydeendadon/status/1973837315476013164\"\n",
      "citations: \"https://x.com/mydeendadon/status/1973837734793130255\"\n",
      "citations: \"https://x.com/spyda247/status/1973832793827184645\"\n",
      "citations: \"https://x.com/mydeendadon/status/1973837909859185142\"\n",
      "citations: \"https://x.com/Zinzy2024Zinzy/status/1973823007270580626\"\n",
      "citations: \"https://x.com/KuteyiOlawale/status/1973841608660713534\"\n",
      "citations: \"https://dailytrust.com/student-drown-teachers-beaten-in-ogun-school-riot/\"\n",
      "citations: \"https://dailytrust.com/health-workers-in-gombe-get-new-salary-structure/\"\n",
      "citations: \"https://dailytrust.com/2027-why-atiku-is-yet-to-declare-aide/\"\n",
      "citations: \"https://dailytrust.com/erratic-power-hinders-businesses-in-kaduna-kano-katsina/\"\n",
      "citations: \"https://dailytrust.com/nigeria-at-65/\"\n",
      "citations: \"https://dailytrust.com/veteran-golfer-yusuf-crowned-2025-tyb-igrcc-independence-tournament-champion/\"\n",
      "citations: \"https://dailytrust.com/okpebholo-splashes-n10m-on-team-edo-for-nyg-feat/\"\n",
      "settings {\n",
      "  parallel_tool_calls: true\n",
      "  reasoning_effort: EFFORT_MEDIUM\n",
      "  temperature: 0\n",
      "  search_parameters {\n",
      "    mode: ON_SEARCH_MODE\n",
      "    from_date {\n",
      "      seconds: 1759430674\n",
      "      nanos: 393573000\n",
      "    }\n",
      "    to_date {\n",
      "      seconds: 1759459474\n",
      "      nanos: 393589000\n",
      "    }\n",
      "    return_citations: true\n",
      "    max_search_results: 30\n",
      "    sources {\n",
      "      web {\n",
      "        country: \"NG\"\n",
      "        safe_search: true\n",
      "      }\n",
      "    }\n",
      "    sources {\n",
      "      news {\n",
      "        country: \"NG\"\n",
      "        safe_search: true\n",
      "      }\n",
      "    }\n",
      "    sources {\n",
      "      x {\n",
      "      }\n",
      "    }\n",
      "    sources {\n",
      "      rss {\n",
      "        links: \"https://dailytrust.com/feed/\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "here\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Example integration after you parse your API response\n",
    "try:\n",
    "    with open(\"Grok/key.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    grok_api_key = config[\"grok_api_key\"]\n",
    "    #convex_api_key = config[\"convex_api_key\"]\n",
    "    \n",
    "    news_data = fetch_security_news(grok_api_key)\n",
    "    print(news_data)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ùå Could not parse response content as JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567c1d9-c32e-4fa2-b08e-46937996136f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
